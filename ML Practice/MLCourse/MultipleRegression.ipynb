{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a small little data set of Blue Book car values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('http://cdn.sundog-soft.com/Udemy/DataScience/cars.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "df166=df[['Mileage','Price']]\n",
    "#print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins =  np.arange(0,50000,10000)\n",
    "groups = df1.groupby(pd.cut(df1['Mileage'],bins)).mean()\n",
    "print(groups.head())\n",
    "groups['Price'].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas to split up this matrix into the feature vectors we're interested in, and the value we're trying to predict.\n",
    "\n",
    "Note how we are avoiding the make and model; regressions don't work well with ordinal values, unless you can convert them into some numerical order that makes sense somehow.\n",
    "\n",
    "Let's scale our feature data into the same range so we can easily compare the coefficients we end up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mileage  Cylinder     Doors\n",
      "0   -1.417485   0.52741  0.556279\n",
      "1   -1.305902   0.52741  0.556279\n",
      "2   -0.810128   0.52741  0.556279\n",
      "3   -0.426058   0.52741  0.556279\n",
      "4    0.000008   0.52741  0.556279\n",
      "..        ...       ...       ...\n",
      "799 -0.439853   0.52741  0.556279\n",
      "800 -0.089966   0.52741  0.556279\n",
      "801  0.079605   0.52741  0.556279\n",
      "802  0.750446   0.52741  0.556279\n",
      "803  1.932565   0.52741  0.556279\n",
      "\n",
      "[804 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Price</td>      <th>  R-squared (uncentered):</th>      <td>   0.064</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.060</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   18.11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 02 Apr 2020</td> <th>  Prob (F-statistic):</th>          <td>2.23e-11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:11:25</td>     <th>  Log-Likelihood:    </th>          <td> -9207.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   804</td>      <th>  AIC:               </th>          <td>1.842e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   801</td>      <th>  BIC:               </th>          <td>1.843e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mileage</th>  <td>-1272.3412</td> <td>  804.623</td> <td>   -1.581</td> <td> 0.114</td> <td>-2851.759</td> <td>  307.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cylinder</th> <td> 5587.4472</td> <td>  804.509</td> <td>    6.945</td> <td> 0.000</td> <td> 4008.252</td> <td> 7166.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Doors</th>    <td>-1404.5513</td> <td>  804.275</td> <td>   -1.746</td> <td> 0.081</td> <td>-2983.288</td> <td>  174.185</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>157.913</td> <th>  Durbin-Watson:     </th> <td>   0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 257.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.278</td>  <th>  Prob(JB):          </th> <td>1.20e-56</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.074</td>  <th>  Cond. No.          </th> <td>    1.03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                  Price   R-squared (uncentered):                   0.064\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.060\n",
       "Method:                 Least Squares   F-statistic:                              18.11\n",
       "Date:                Thu, 02 Apr 2020   Prob (F-statistic):                    2.23e-11\n",
       "Time:                        08:11:25   Log-Likelihood:                         -9207.1\n",
       "No. Observations:                 804   AIC:                                  1.842e+04\n",
       "Df Residuals:                     801   BIC:                                  1.843e+04\n",
       "Df Model:                           3                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Mileage    -1272.3412    804.623     -1.581      0.114   -2851.759     307.077\n",
       "Cylinder    5587.4472    804.509      6.945      0.000    4008.252    7166.642\n",
       "Doors      -1404.5513    804.275     -1.746      0.081   -2983.288     174.185\n",
       "==============================================================================\n",
       "Omnibus:                      157.913   Durbin-Watson:                   0.008\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              257.529\n",
       "Skew:                           1.278   Prob(JB):                     1.20e-56\n",
       "Kurtosis:                       4.074   Cond. No.                         1.03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "X = df[['Mileage', 'Cylinder', 'Doors']]\n",
    "y = df['Price']\n",
    "\n",
    "X[['Mileage', 'Cylinder', 'Doors']] = scale.fit_transform(X[['Mileage', 'Cylinder', 'Doors']].values)\n",
    "\n",
    "print (X)\n",
    "\n",
    "est = sm.OLS(y, X).fit()\n",
    "\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of coefficients above gives us the values to plug into an equation of form:\n",
    "    B0 + B1 * Mileage + B2 * cylinders + B3 * doors\n",
    "    \n",
    "In this example, it's pretty clear that the number of cylinders is more important than anything based on the coefficients.\n",
    "\n",
    "Could we have figured that out earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doors\n",
       "2    23807.135520\n",
       "4    20580.670749\n",
       "Name: Price, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.groupby(df.Doors).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, more doors does not mean a higher price! (Maybe it implies a sport car in some cases?) So it's not surprising that it's pretty useless as a predictor here. This is a very small data set however, so we can't really read much meaning into it.\n",
    "\n",
    "How would you use this to make an actual prediction? Start by scaling your multiple feature variables into the same scale used to train the model, then just call est.predict() on the scaled features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.07256589 1.96971667 0.55627894]]\n",
      "[6315.01330583]\n"
     ]
    }
   ],
   "source": [
    "scaled = scale.transform([[45000, 8, 4]])\n",
    "print(scaled)\n",
    "predicted = est.predict(scaled[0])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mess around with the fake input data, and see if you can create a measurable influence of number of doors on price. Have some fun with it - why stop at 4 doors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
